{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7125f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Prashant\\\\Downloads\\\\IIM Indore\\\\Linear Algebra'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22146061",
   "metadata": {},
   "source": [
    "# Vector Dot Product "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2da64",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "The dot product (also known as the scalar product) of two vectors is a fundamental operation in both physics and data science. Mathematically, if you have two vectors $ \\mathbf{a} $ and $ \\mathbf{b} $ of dimensions $ n $, their dot product is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\cdots + a_nb_n = \\sum_{i=1}^n a_i b_i\n",
    "$$\n",
    "\n",
    "The result is a scalar (hence \"scalar product\"), and it measures the magnitude of one vector in the direction of the other. The dot product can also be expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "where $ \\|\\mathbf{a}\\| $ and $ \\|\\mathbf{b}\\| $ are the magnitudes (or norms) of vectors $ \\mathbf{a} $ and $ \\mathbf{b} $, and $ \\theta $ is the angle between them.\n",
    "\n",
    "### Applications in Data Science and Physics\n",
    "\n",
    "1. **Cosine Similarity**:\n",
    "   - In data science, particularly in natural language processing and information retrieval, cosine similarity is a common metric derived from the dot product. It measures the cosine of the angle between two vectors, thus indicating how similar they are irrespective of their magnitude. This is widely used in document similarity and text classification.\n",
    "\n",
    "2. **Physics**:\n",
    "   - In physics, the dot product is used to calculate work done when a force is applied to an object and the object moves. The work done $ W $ by a force $ \\mathbf{F} $ moving an object through a displacement $ \\mathbf{d} $ is $ W = \\mathbf{F} \\cdot \\mathbf{d} $.\n",
    "   - It's also used in calculating the electric flux through a surface.\n",
    "\n",
    "3. **Projection of Vectors**:\n",
    "   - The projection of vector $ \\mathbf{a} $ onto $ \\mathbf{b} $ can be calculated using the dot product, which is critical in various geometric calculations in computer graphics and simulations.\n",
    "\n",
    "4. **Feature Similarity in Machine Learning**:\n",
    "   - Dot products are fundamental to algorithms like support vector machines (SVMs) and neural networks, where the similarity or distance between feature vectors directly influences the learning algorithm.\n",
    "   \n",
    "5. **Computer Graphics and Vision:**\n",
    "\n",
    "    3D Rendering and Lighting: The dot product is essential in computer graphics, particularly in determining the angle between light sources and surfaces, known as \"shading.\" When rendering 3D objects, the dot product helps calculate how light reflects on surfaces, influencing the object's appearance.\n",
    "    Facial Recognition: In computer vision, the dot product can measure the similarity between different images or shapes. It's useful in facial recognition technologies, comparing the alignment and features of faces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f80eb5",
   "metadata": {},
   "source": [
    "### Python Example: Calculating Dot Product and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61c9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63febaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 32\n"
     ]
    }
   ],
   "source": [
    "# Calculate dot product\n",
    "dot_product = np.dot(a, b)\n",
    "print(\"Dot Product:\", dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc41c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate magnitudes of the vectors\n",
    "norm_a = np.linalg.norm(a)\n",
    "norm_b = np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c478c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "cosine_similarity = dot_product / (norm_a * norm_b)\n",
    "print(\"Cosine Similarity:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a876b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Note: the function returns a similarity matrix, where each cell [i][j] represents the cosine similarity between vectors i and j.\n",
    "similarity = cosine_similarity(a.reshape(1,-1), b.reshape(1,-1))\n",
    "print(\"Cosine similarity:\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9dda67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f191142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd337ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "763c7ea8",
   "metadata": {},
   "source": [
    "Let’s delve deeper into the applications of the dot product in various fields, providing mathematical formulas and insights into their practical uses:\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a metric used primarily in text analysis and information retrieval to determine how similar two documents or sets of words are, regardless of their size. Mathematically, cosine similarity between two vectors $ \\mathbf{a} $ and $ \\mathbf{b} $ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{a} \\cdot \\mathbf{b} $ is the dot product of the vectors, and $ \\|\\mathbf{a}\\| $ and $ \\|\\mathbf{b}\\| $ are the Euclidean norms (or magnitudes) of these vectors. This formula effectively measures the cosine of the angle between the two vectors, providing a value between -1 and 1. A cosine similarity of 1 means the vectors are perfectly aligned (indicating similarity), 0 means they are orthogonal (no similarity), and -1 means they are diametrically opposed.\n",
    "\n",
    "**Applications in NLP**:\n",
    "- Comparing text documents to gauge similarity for plagiarism detection or document clustering.\n",
    "- Calculating similarity scores between words in models like Word2Vec to find synonyms or semantically similar words.\n",
    "\n",
    "### Physics: Work Done and Electric Flux\n",
    "\n",
    "1. **Work Done by a Force**:\n",
    "   In physics, work done $ W $ by a force $ \\mathbf{F} $ when moving an object through a displacement $ \\mathbf{d} $ is given by:\n",
    "   \n",
    "   $$\n",
    "   W = \\mathbf{F} \\cdot \\mathbf{d}\n",
    "   $$\n",
    "\n",
    "   This formula means the work done is the component of the force in the direction of displacement times the magnitude of the displacement. If the force is perpendicular to the displacement, no work is done.\n",
    "\n",
    "2. **Electric Flux**:\n",
    "   The electric flux through a surface can be calculated using the dot product when considering the electric field $ \\mathbf{E} $ and an infinitesimal area vector $ \\mathbf{A} $ (which is normal to the surface and whose magnitude is the area of the surface):\n",
    "\n",
    "   $$\n",
    "   \\Phi = \\mathbf{E} \\cdot \\mathbf{A}\n",
    "   $$\n",
    "\n",
    "   This calculation helps determine how much field passes through a given area, critical in Gauss's Law applications.\n",
    "\n",
    "### Projection of Vectors\n",
    "\n",
    "The projection of vector $ \\mathbf{a} $ onto another vector $ \\mathbf{b} $ is a vector that lies on $ \\mathbf{b} $ and is obtained by:\n",
    "\n",
    "$$\n",
    "\\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{b} \\cdot \\mathbf{b}} \\mathbf{b}\n",
    "$$\n",
    "\n",
    "This operation is useful in computer graphics for shadow rendering, in physics for resolving forces into components, and in machine learning for feature decomposition.\n",
    "\n",
    "### Feature Similarity in Machine Learning (Advanced)\n",
    "\n",
    "In machine learning, particularly in algorithms that use kernel methods like SVM or in neural networks, the dot product is used to compute the similarity between feature vectors. In kernel SVMs, for example, the dot product is used to calculate the kernel function, which effectively measures the inner product of vectors in a higher-dimensional space without explicitly mapping them to that space. This is foundational in techniques like the polynomial kernel, sigmoid kernel, and others.\n",
    "\n",
    "The dot product's ability to abstract and simplify calculations while providing meaningful metrics for similarity, work, and projection makes it an invaluable tool across data science, physics, and engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4126eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2fd633",
   "metadata": {},
   "source": [
    "### Cosine Similarity in Word Embeddings\n",
    "\n",
    "**Word embeddings** are a powerful method used in natural language processing (NLP) to represent text in a numerical format where words with similar meanings have similar representations. These embeddings typically reside in high-dimensional vector spaces, and cosine similarity is a preferred metric to quantify the semantic similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71a257",
   "metadata": {},
   "source": [
    "#### Example with Word Embeddings\n",
    "\n",
    "Let’s consider a hypothetical example using word embeddings. Suppose we have embeddings (vector representations) for three words: \"king\", \"queen\", and \"man\". In a simplistic scenario, the word \"king\" might be represented by the vector $[3, 2, 1]$, \"queen\" by $[3, 2.1, 0.9]$, and \"man\" by $[1, 0.9, 2]$.\n",
    "\n",
    "Here’s how you can calculate the cosine similarity between these words to determine their semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ba2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings\n",
    "king = np.array([3, 2, 1])\n",
    "queen = np.array([3, 2.5, 0.8])\n",
    "man = np.array([1, 0.9, 2.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b8d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a2b979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'king' and 'queen': 0.9922834522951708\n",
      "Cosine Similarity between 'king' and 'man': 0.7394177649636069\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities\n",
    "similarity_king_queen = cosine_similarity(king, queen)\n",
    "similarity_king_man = cosine_similarity(king, man)\n",
    "\n",
    "print(\"Cosine Similarity between 'king' and 'queen':\", similarity_king_queen)\n",
    "print(\"Cosine Similarity between 'king' and 'man':\", similarity_king_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e85e7",
   "metadata": {},
   "source": [
    "The high cosine similarity between \"king\" and \"queen\" suggests a close semantic relationship, likely because these words often appear in similar contexts and have related meanings. The lower similarity between \"king\" and \"man\" indicates a less direct relationship, reflecting their broader and more diverse usage in language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ba9a0",
   "metadata": {},
   "source": [
    "### Cosine Similarity in Recommender Systems\n",
    "\n",
    "**Recommender Systems** often utilize cosine similarity to measure the similarity between user or item profiles. This approach is particularly common in content-based and collaborative filtering methods.\n",
    "\n",
    "#### Example in a Recommender System\n",
    "\n",
    "Consider a content-based recommender system where items (e.g., movies, articles) are represented by feature vectors based on their content attributes (genres, keywords, themes). The system recommends items that are most similar to what a user liked before, calculated using the cosine similarity between the user's preferred item vectors and other available item vectors.\n",
    "\n",
    "Here’s a simplified example of how cosine similarity might be used in a recommender system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0cea78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define item feature vectors (e.g., genre scores for movies)\n",
    "item1 = np.array([0.9, 0.1, 0])  # Mostly action, little romance\n",
    "item2 = np.array([0.1, 0.9, 0])  # Mostly romance\n",
    "user_likes = np.array([0.85, 0.15, 0])  # User likes action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db66ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Item 1: 0.9979517409161514\n",
      "Similarity with Item 2: 0.2814735679507094\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarity_item1 = cosine_similarity(user_likes, item1)\n",
    "similarity_item2 = cosine_similarity(user_likes, item2)\n",
    "\n",
    "print(\"Similarity with Item 1:\", similarity_item1)\n",
    "print(\"Similarity with Item 2:\", similarity_item2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327ad87",
   "metadata": {},
   "source": [
    "The system would recommend Item 1 over Item 2 to this user, as the cosine similarity is higher, suggesting a closer match to the user's preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045a5f2",
   "metadata": {},
   "source": [
    "To demonstrate the use of cosine similarity for measuring similarity between word vectors using the spaCy library, you first need to have spaCy installed and download a model that includes word vectors. spaCy offers several models for English, among other languages, some of which include pre-trained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37ffaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e5d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08fe26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium English model which includes word vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Words to compare\n",
    "word1 = nlp(\"king\")\n",
    "word2 = nlp(\"queen\")\n",
    "word3 = nlp(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a54b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vectors\n",
    "vector1 = word1.vector\n",
    "vector2 = word2.vector\n",
    "vector3 = word3.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b0d18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'king' and 'queen': 0.6109\n",
      "Cosine similarity between 'king' and 'apple': 0.1952\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarity_king_queen = cosine_similarity(vector1, vector2)\n",
    "similarity_king_apple = cosine_similarity(vector1, vector3)\n",
    "\n",
    "print(f\"Cosine similarity between 'king' and 'queen': {similarity_king_queen:.4f}\")\n",
    "print(f\"Cosine similarity between 'king' and 'apple': {similarity_king_apple:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e235d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
